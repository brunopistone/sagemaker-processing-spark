{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Probabilistic Forecasting - Electricity\n",
    "\n",
    "This notebook demonstrates how to perform Data Analysis and Preparation Engineering with Amazon SageMaker Studio using AWS Glue Interactive Session.\n",
    "\n",
    "Using this notebook, we can execute cells in order to read data, visualize, and perform transformations using PySpark with AWS Glue Interactice Session.\n",
    "\n",
    "Let's start preparing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "The data set (Electricity Price Forecasting) was downloaded from [Kaggle](https://www.kaggle.com/code/dimitriosroussis/electricity-price-forecasting-with-dnns-eda/data).\n",
    "\n",
    "This dataset is using the past values of the electricity price as well as those of another features related to energy generation and weather conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Step 1 - Import Modules\n",
    "\n",
    "Here weâ€™ll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<aws_profile>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:32.246368Z",
     "start_time": "2023-07-31T18:06:32.226539Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.spark.processing import PySparkProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:32.729496Z",
     "start_time": "2023-07-31T18:06:32.532323Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create a SageMaker Session and save the default region and the execution role in some Python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:44.906370Z",
     "start_time": "2023-07-31T18:06:44.859606Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:45.740421Z",
     "start_time": "2023-07-31T18:06:45.218973Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Prepare data and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/syntetic_data_energy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/syntetic_data_weather.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"./data/output\")\n",
    "\n",
    "for file_path in output_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        # Create S3 key by replacing local path structure\n",
    "        relative_path = file_path.relative_to(output_dir)\n",
    "        s3_key = f\"electricity-forecasting/data/input/{relative_path}\"\n",
    "\n",
    "        print(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n",
    "        s3_client.upload_file(str(file_path), bucket_name, s3_key)\n",
    "\n",
    "print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Step 3 - Upload Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:46.876075Z",
     "start_time": "2023-07-31T18:06:46.865723Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "script_location = \"electricity-forecasting/code/processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:47.982388Z",
     "start_time": "2023-07-31T18:06:47.212277Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the\n",
    "# clean the buckets first\n",
    "s3_client.delete_object(Bucket=bucket_name, Key=script_location)\n",
    "\n",
    "code_path = sagemaker_session.upload_data('./code', key_prefix=script_location)\n",
    "\n",
    "code_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Step 4 - Run the processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "By using [PySparkProcessor](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.html), we can provide to the Amazon SageMaker Job the execution PySpark scripts in distributed data processing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:49.358745Z",
     "start_time": "2023-07-31T18:06:48.713274Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "! pygmentize ./code/processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Global Parameters\n",
    "\n",
    "In order to allow users to execute the SageMaker Processing Job locally, we are defining the variable `local_mode`. If you want to test the local mode capability, please put the variable to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:07:31.511122Z",
     "start_time": "2023-07-31T18:07:31.508568Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Change spark_image_uri based on your region. Visit https://github.com/aws/sagemaker-spark-container/releases\n",
    "spark_image_uri = \"173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.5-cpu-py312-v1.0\"\n",
    "\n",
    "processing_code = \"electricity-forecasting/code/processing\"\n",
    "processing_input_files_path = \"electricity-forecasting/data/input\"\n",
    "processing_output_files_path = \"electricity-forecasting/data/output\"\n",
    "\n",
    "processing_instance_count = 2\n",
    "processing_instance_type = \"ml.m5.12xlarge\"\n",
    "\n",
    "spark_configurations = [\n",
    "    {\n",
    "        \"Classification\":\"spark-defaults\",\n",
    "        \"Properties\":{\n",
    "            \"spark.executor.cores\": 5,\n",
    "            \"spark.driver.cores\": 5,\n",
    "            \"spark.executor.memory\": \"35g\",\n",
    "            \"spark.executor.memoryOverhead\": \"3g\",\n",
    "            \"spark.driver.memory\": \"35g\",\n",
    "            \"spark.executor.instances\": 17,\n",
    "            \"spark.sql.parquet.fs.optimized.comitter.optimization-enabled\": True\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define the `PySparkProcessor` object.\n",
    "\n",
    "### Update:\n",
    "\n",
    "From the container version `sagemaker-spark-processing:3.3-cpu-py39-v1.2`, SageMaker Spark Containers are providing an automated optimized Spark configuration. For using it, provide the environment variable `AWS_SPARK_CONFIG_MODE = \"2\"`\n",
    "\n",
    "```\n",
    "env={\n",
    "    \"AWS_SPARK_CONFIG_MODE\": \"2\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:07:32.700863Z",
     "start_time": "2023-07-31T18:07:32.675318Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "processor = PySparkProcessor(\n",
    "    image_uri=spark_image_uri,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\n",
    "        \"AWS_SPARK_CONFIG_MODE\": \"2\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:07:53.188881Z",
     "start_time": "2023-05-05T20:48:01.820519Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    \"./code/processing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"input\",\n",
    "            source=\"s3://{}/{}/\".format(bucket_name, processing_input_files_path),\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"scripts\",\n",
    "            source=\"s3://{}/{}\".format(bucket_name, processing_code),\n",
    "            destination=\"/opt/ml/processing/input/code/scripts\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"output\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=\"s3://{}/{}\".format(bucket_name, processing_output_files_path))\n",
    "    ],\n",
    "    #configuration=spark_configurations,\n",
    "    spark_event_logs_s3_uri=\"s3://{}/electricity-forecasting/logs\".format(bucket_name),\n",
    "    wait=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
