{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Probabilistic Forecasting - Electricity\n",
    "\n",
    "This notebook demonstrates how to perform Data Analysis and Preparation Engineering with Amazon SageMaker Studio using AWS Glue Interactive Session.\n",
    "\n",
    "Using this notebook, we can execute cells in order to read data, visualize, and perform transformations using PySpark with AWS Glue Interactice Session.\n",
    "\n",
    "Let's start preparing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "The data set (Electricity Price Forecasting) was downloaded from [Kaggle](https://www.kaggle.com/code/dimitriosroussis/electricity-price-forecasting-with-dnns-eda/data).\n",
    "\n",
    "This dataset is using the past values of the electricity price as well as those of another features related to energy generation and weather conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Step 1 - Import Modules\n",
    "\n",
    "Here weâ€™ll import some libraries and define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<aws_profile>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:32.246368Z",
     "start_time": "2023-07-31T18:06:32.226539Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.modules.configs import (\n",
    "    Compute,\n",
    "    OutputDataConfig,\n",
    "    SourceCode,\n",
    "    StoppingCondition,\n",
    ")\n",
    "from sagemaker.modules.train import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:32.729496Z",
     "start_time": "2023-07-31T18:06:32.532323Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create a SageMaker Session and save the default region and the execution role in some Python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:44.906370Z",
     "start_time": "2023-07-31T18:06:44.859606Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:45.740421Z",
     "start_time": "2023-07-31T18:06:45.218973Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "\n",
    "bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Prepare data and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/syntetic_data_energy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/syntetic_data_weather.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"./data/output\")\n",
    "\n",
    "for file_path in output_dir.rglob(\"*\"):\n",
    "    if file_path.is_file():\n",
    "        # Create S3 key by replacing local path structure\n",
    "        relative_path = file_path.relative_to(output_dir)\n",
    "        s3_key = f\"electricity-forecasting/data/input/{relative_path}\"\n",
    "\n",
    "        print(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n",
    "        s3_client.upload_file(str(file_path), bucket_name, s3_key)\n",
    "\n",
    "print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Step 3 - Run the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "By using [ModelTrainer](https://sagemaker.readthedocs.io/en/stable/api/training/model_trainer.html), we can provide to the Amazon SageMaker Job the execution PySpark scripts in distributed data processing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:06:49.358745Z",
     "start_time": "2023-07-31T18:06:48.713274Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "! pygmentize ./code/processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Global Parameters\n",
    "\n",
    "In order to allow users to execute the SageMaker Processing Job locally, we are defining the variable `local_mode`. If you want to test the local mode capability, please put the variable to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:07:31.511122Z",
     "start_time": "2023-07-31T18:07:31.508568Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Change spark_image_uri based on your region. Visit https://github.com/aws/sagemaker-spark-container/releases\n",
    "spark_image_uri = \"173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.5-cpu-py312-v1.0\"\n",
    "\n",
    "processing_code = \"electricity-forecasting/code/processing\"\n",
    "processing_input_files_path = \"electricity-forecasting/data/input\"\n",
    "processing_output_files_path = \"electricity-forecasting/data/output\"\n",
    "\n",
    "processing_instance_count = 2\n",
    "processing_instance_type = \"ml.m5.12xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_configurations = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\n",
    "            \"spark.executor.cores\": 5,\n",
    "            \"spark.driver.cores\": 5,\n",
    "            \"spark.executor.memory\": \"35g\",\n",
    "            \"spark.executor.memoryOverhead\": \"3g\",\n",
    "            \"spark.driver.memory\": \"35g\",\n",
    "            \"spark.executor.instances\": 17,\n",
    "            \"spark.sql.parquet.fs.optimized.comitter.optimization-enabled\": True,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# Write spark_configurations to JSON file\n",
    "with open(\"configuration.json\", \"w\") as f:\n",
    "    json.dump(spark_configurations, f)\n",
    "\n",
    "# Upload to S3\n",
    "if default_prefix:\n",
    "    input_path = (\n",
    "        f\"s3://{bucket_name}/{default_prefix}/sagemaker-training-spark/configurations\"\n",
    "    )\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/sagemaker-training-spark/configurations\"\n",
    "\n",
    "spark_config_s3_path = S3Uploader.upload(\n",
    "    local_path=\"configuration.json\", desired_s3_uri=f\"{input_path}/config\"\n",
    ")\n",
    "\n",
    "os.remove(\"configuration.json\")\n",
    "\n",
    "print(f\"Spark config uploaded to:\")\n",
    "print(spark_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define the `ModelTrainer` object.\n",
    "\n",
    "### Update:\n",
    "\n",
    "From the container version `sagemaker-spark-processing:3.3-cpu-py39-v1.2`, SageMaker Spark Containers are providing an automated optimized Spark configuration. For using it, provide the environment variable `AWS_SPARK_CONFIG_MODE = \"2\"`\n",
    "\n",
    "```\n",
    "env={\n",
    "    \"AWS_SPARK_CONFIG_MODE\": \"2\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T18:07:32.700863Z",
     "start_time": "2023-07-31T18:07:32.675318Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--local-spark-event-logs-dir\",\n",
    "    \"/opt/ml/output/data/spark-events/\",\n",
    "    \"/opt/ml/input/data/code/processing.py\",\n",
    "]\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./code\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    command=f\"smspark-submit {' '.join(args)}\"\n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    keep_alive_period_in_seconds=0,\n",
    ")\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"process-spark-job\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{processing_output_files_path}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{processing_output_files_path}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=spark_image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\n",
    "    environment={\n",
    "        \"IS_TRAINING_JOB\": \"true\", \n",
    "        \"AWS_SPARK_CONFIG_MODE\": \"2\"\n",
    "    },\n",
    "    output_data_config=OutputDataConfig(\n",
    "        s3_output_path=output_path, \n",
    "        compression_type=\"NONE\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:07:53.188881Z",
     "start_time": "2023-05-05T20:48:01.820519Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData, S3DataSource\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"input\",\n",
    "    data_source=S3DataSource(\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_uri=\"s3://{}/{}/\".format(bucket_name, processing_input_files_path),\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "config_input = InputData(\n",
    "    channel_name=\"conf\",\n",
    "    data_source=spark_config_s3_path,  # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [train_input, config_input]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(input_data_config=data, wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
